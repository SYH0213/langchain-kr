{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 예시: 프롬프트 + 모델 + 출력 파서\n",
    "\n",
    "가장 기본적이고 일반적인 사용 사례는 prompt 템플릿과 모델을 함께 연결하는 것입니다. 이것이 어떻게 작동하는지 보기 위해, 각 나라별 수도를 물어보는 Chain을 생성해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH01-Basic\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH01-Basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 템플릿의 활용\n",
    "\n",
    "`PromptTemplate`\n",
    "\n",
    "- 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\n",
    "- 사용법\n",
    "  - `template`: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 `{}`는 변수를 나타냅니다.\n",
    "  - `input_variables`: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\n",
    "\n",
    "`input_variables`\n",
    "\n",
    "- input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response  # 스트리밍 출력\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_template()` 메소드를 사용하여 PromptTemplate 객체 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template 정의\n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# from_template 메소드를 이용하여 PromptTemplate 객체 생성\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+2=3\n"
     ]
    }
   ],
   "source": [
    "#python\n",
    "print(\"{}+{}={}\".format(1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = prompt_template.format(country=\"대한민국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = prompt_template.format(country=\"미국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain 생성\n",
    "\n",
    "### LCEL(LangChain Expression Language)\n",
    "\n",
    "![lcel.png](./images/lcel.png)\n",
    "\n",
    "여기서 우리는 LCEL을 사용하여 다양한 구성 요소를 단일 체인으로 결합합니다\n",
    "\n",
    "```\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "`|` 기호는 [unix 파이프 연산자](<https://en.wikipedia.org/wiki/Pipeline_(Unix)>)와 유사하며, 서로 다른 구성 요소를 연결하고 한 구성 요소의 출력을 다음 구성 요소의 입력으로 전달합니다.\n",
    "\n",
    "이 체인에서 사용자 입력은 프롬프트 템플릿으로 전달되고, 그런 다음 프롬프트 템플릿 출력은 모델로 전달됩니다. 각 구성 요소를 개별적으로 살펴보면 무슨 일이 일어나고 있는지 이해할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 를 PromptTemplate 객체로 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0.1)\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invoke() 호출\n",
    "\n",
    "- python 딕셔너리 형태로 입력값을 전달합니다.(키: 값)\n",
    "- invoke() 함수 호출 시, 입력값을 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 딕셔너리에 주제를 '인공지능 모델의 학습 원리'으로 설정합니다.\n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"물론입니다! 인공지능 모델의 학습 원리를 쉽게 설명해 드릴게요.\\n\\n1. **데이터 수집**: 먼저, 인공지능이 배우기 위해 많은 예제(데이터)를 모아요. 예를 들어, 고양이 사진을 많이 보여주는 것처럼요.\\n\\n2. **모델 설계**: 그 다음, 컴퓨터가 데이터를 이해하고 학습할 수 있도록 '모델'이라는 구조를 만듭니다. 이 모델은 사람의 뇌와 비슷하게 여러 층으로 이루어진 '신경망'일 수 있어요.\\n\\n3. **학습 과정**:\\n   - 모델은 처음에는 아무것도 모르기 때문에, 예를 들어 고양이 사진인지 아닌지 구별하지 못해요.\\n   - 모델이 사진을 보고 '이게 고양이일까 아니면 아니까'라고 추측합니다.\\n   - 그리고 정답(이 사진은 고양이야!)과 비교해서, 틀렸다면 어디가 잘못됐는지 계산해요.\\n   - 그 차이(오차)를 줄이기 위해 모델의 내부 값(가중치)을 조금씩 조정합니다.\\n   \\n4. **반복 학습**:\\n   - 이 과정을 수천, 수만 번 반복하면서, 모델은 점점 더 정확하게 사진을 구별할 수 있게 돼요.\\n   \\n5. **완성**:\\n   - 충분히 학습이 되면, 새로운 사진이 들어왔을 때도 고양이인지 아닌지 잘 맞출 수 있게 되는 거죠.\\n\\n요약하자면, 인공지능은 많은 예제와 반복 학습을 통해 '이것이 정답이다'라고 스스로 배우는 과정입니다. 이렇게 해서 점점 더 똑똑해지는 거예요!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 384, 'prompt_tokens': 22, 'total_tokens': 406, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_38343a2f8f', 'id': 'chatcmpl-C4KG5LBilUXdIddLaaGf74jC93FaX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b2696112-54d7-4fcb-98dc-cee90297a606-0', usage_metadata={'input_tokens': 22, 'output_tokens': 384, 'total_tokens': 406, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달합니다.\n",
    "# 이를 통해 AI 모델이 생성한 메시지를 반환합니다.\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 스트리밍을 출력하는 예시 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론입니다! 인공지능 모델의 학습 원리를 쉽게 설명해 드릴게요.\n",
      "\n",
      "1. **데이터 수집**: 먼저, 인공지능이 배울 수 있도록 많은 예제(데이터)를 모아요. 예를 들어, 고양이 사진과 고양이 이름이 적힌 데이터가 있겠죠.\n",
      "\n",
      "2. **모델 설계**: 그런 다음, 이 데이터를 이해하고 학습할 수 있는 '모델'이라는 수학적 구조를 만듭니다. 이 모델은 사람의 뇌처럼 정보를 처리하는 방식과 비슷하게 만들어질 수 있어요.\n",
      "\n",
      "3. **학습 과정**:\n",
      "   - 모델은 처음에는 아무것도 모른 상태예요.\n",
      "   - 데이터를 보여주면서, 모델이 예측한 결과와 실제 정답(예를 들어, 사진이 고양이인지 아닌지)을 비교해요.\n",
      "   - 차이(오차)를 계산하고, 이 오차를 줄이기 위해 모델의 내부 값을 조금씩 조정해요. 이 과정을 여러 번 반복하면서 모델이 점점 더 정확하게 예측하게 되는 거죠.\n",
      "\n",
      "4. **최적화**: 이때 '최적화 알고리즘'이라는 것이 사용돼서, 오차를 최소화하는 방향으로 모델의 내부 값을 조정하는 역할을 해요.\n",
      "\n",
      "5. **학습 완료**: 여러 번 반복해서 학습하면, 모델은 새로운 데이터(예를 들어, 새로운 고양이 사진)도 잘 분류할 수 있게 돼요.\n",
      "\n",
      "요약하자면, 인공지능 모델은 많은 데이터를 보고, 예측과 정답의 차이를 줄이기 위해 계속 조정하면서 점점 더 똑똑해지는 과정입니다."
     ]
    }
   ],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream(input)\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력파서(Output Parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain 에 출력파서를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트, 모델, 출력 파서를 연결하여 처리 체인을 구성합니다.\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"물론입니다! 인공지능 모델의 학습 원리를 쉽게 설명해 드릴게요.\\n\\n1. **데이터 수집**: 먼저, 인공지능이 배우기 위해 많은 예제(데이터)를 모아요. 예를 들어, 고양이 사진을 많이 보여주는 거죠.\\n\\n2. **모델 설계**: 그런 데이터를 이해하고 학습할 수 있도록 수학적인 구조(모델)를 만듭니다. 이 구조는 사람의 뇌와 비슷하게 여러 '뉴런'이 연결된 것처럼 생각할 수 있어요.\\n\\n3. **학습 과정**:\\n   - **예측하기**: 모델은 처음에는 무작위로 예측을 해요. 예를 들어, 사진이 고양이인지 강아지인지 맞추려고 시도하는 거죠.\\n   - **오차 계산**: 예측이 틀리면, 얼마나 틀렸는지(오차)를 계산해요.\\n   - **수정하기**: 오차를 줄이기 위해 모델의 내부 값(가중치)을 조금씩 조정해요. 이 과정을 반복하면서 점점 더 정확한 예측을 하게 되는 거죠.\\n\\n4. **반복 학습**: 이 과정을 여러 번 반복하면서 모델은 점점 더 데이터를 잘 이해하게 되고, 새로운 데이터에 대해서도 올바르게 예측할 수 있게 돼요.\\n\\n요약하자면, 인공지능은 많은 데이터를 보고, 예측을 하고, 틀리면 고치면서 점점 더 똑똑해지는 과정입니다. 이렇게 해서 우리가 원하는 일을 할 수 있도록 학습하는 거예요!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 객체의 invoke 메서드를 사용하여 input을 전달합니다.\n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\"}\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론입니다! 인공지능 모델의 학습 원리를 쉽게 설명해 드릴게요.\n",
      "\n",
      "1. **데이터 수집**: 먼저, 인공지능이 배우기 위해 많은 예제(데이터)를 모아요. 예를 들어, 고양이 사진과 강아지 사진을 모으는 거죠.\n",
      "\n",
      "2. **모델 설계**: 그런 다음, 이 데이터를 이해하고 분류할 수 있도록 인공지능의 구조(모델)를 만듭니다. 이 구조는 사람의 뇌와 비슷하게 여러 층으로 이루어진 '신경망'이라고 부르기도 해요.\n",
      "\n",
      "3. **학습 과정**:\n",
      "   - 인공지능은 처음에는 아무것도 모르기 때문에, 사진이 고양이인지 강아지인지 맞추기 어렵습니다.\n",
      "   - 그래서, 모델이 예측한 결과와 실제 정답(예를 들어, 사진이 고양이인지 강아지인지)을 비교해서 차이(오차)를 계산해요.\n",
      "   - 이 차이를 줄이기 위해, 모델 내부의 '가중치'라는 값들을 조금씩 조정합니다.\n",
      "   - 이 과정을 여러 번 반복하면서, 모델은 점점 더 정확하게 사진을 구분할 수 있게 됩니다.\n",
      "\n",
      "4. **최적화**: 이때 '경사 하강법' 같은 방법을 사용해서, 오차를 가장 빠르게 줄일 수 있는 방향으로 가중치를 조정합니다.\n",
      "\n",
      "5. **완성**: 충분히 학습이 되면, 인공지능은 새로운 사진이 들어왔을 때도 고양이인지 강아지인지 잘 맞출 수 있게 되는 거죠.\n",
      "\n",
      "요약하자면, 인공지능은 많은 데이터를 보고, 예측과 정답의 차이를 줄이기 위해 내부 값을 계속 조정하면서 '배우는' 과정입니다."
     ]
    }
   ],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream(input)\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 템플릿을 변경하여 적용\n",
    "\n",
    "- 아래의 프롬프트 내용을 얼마든지 **변경** 하여 테스트 해볼 수 있습니다.\n",
    "- `model_name` 역시 변경하여 테스트가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\n",
    "양식은 [FORMAT]을 참고하여 작성해 주세요.\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿을 이용하여 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# ChatOpenAI 챗모델을 초기화합니다.\n",
    "model = ChatOpenAI(model_name=\"gpt-4.1-nano\")\n",
    "\n",
    "# 문자열 출력 파서를 초기화합니다.\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# 공통 시스템 페르소나(권장: 시스템 메시지로 역할 고정)\n",
    "SYSTEM = \"당신은 10년차 영어 선생님입니다. 간결하지만 자연스러운 회화를 만들고, 학습 목적에 맞게 난이도를 조절하세요.\"\n",
    "\n",
    "# (A) 기본형 — 현재 템플릿에 가까움\n",
    "TEMPLATE_BASIC = \"\"\"\n",
    "주어진 상황에 맞는 영어 회화를 작성해 주세요.\n",
    "[요구사항]\n",
    "- 회화 턴 수: {turns}턴\n",
    "- 난이도: {level} (예: 초급, 중급, 고급)\n",
    "- 일상적으로 자연스럽고, 너무 과도한 슬랭은 지양\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "\"\"\"\n",
    "\n",
    "# (B) 학습목표 명시형 — 타겟 표현/문법 강제\n",
    "TEMPLATE_TARGET = \"\"\"\n",
    "학습 목표를 반영한 영어 회화를 작성해 주세요.\n",
    "[요구사항]\n",
    "- 회화 턴 수: {turns}턴\n",
    "- 난이도: {level}\n",
    "- 반드시 다음 요소를 최소 2회 사용: {targets}  # 예: present perfect, request politely, phrasal verb 'come up with'\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "- 사용된 표현 체크리스트:\n",
    "\"\"\"\n",
    "\n",
    "# (C) 평가 포함형 — 자기점검 루브릭 동봉\n",
    "TEMPLATE_RUBRIC = \"\"\"\n",
    "상황에 맞는 영어 회화를 작성하고, 마지막에 간단한 자기평가 루브릭을 포함하세요.\n",
    "[요구사항]\n",
    "- 회화 턴 수: {turns}턴\n",
    "- 난이도: {level}\n",
    "- 어휘 레벨: CEFR {cefr} 기준\n",
    "- 목표: {targets}\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "- 루브릭(자기점검 5점 척도 3항목):\n",
    "\"\"\"\n",
    "\n",
    "# (D) 구조화(JSON) 출력형 — 후처리/자동채점에 유리\n",
    "TEMPLATE_JSON = \"\"\"\n",
    "다음 형식을 엄격히 지켜 JSON으로만 답하세요.\n",
    "형식:\n",
    "{{\n",
    "  \"dialogue\": [{{\"speaker\":\"A\",\"en\":\"...\",\"ko\":\"...\"}}, ...],  # 총 {turns}턴\n",
    "  \"targets_covered\": [\"...\", \"...\"],\n",
    "  \"notes\": \"학습 포인트 요약(한글)\"\n",
    "}}\n",
    "\n",
    "[제약]\n",
    "- 난이도: {level}, CEFR {cefr}\n",
    "- 필수표현: {targets}\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "# OpenAI 패키지 경로는 버전에 따라 다릅니다.\n",
    "# (구버전) from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI  # 최신권장\n",
    "parser = StrOutputParser()\n",
    "\n",
    "def build_chain(template_str, model_name=\"gpt-4.1-nano\", temperature=0.7):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYSTEM),\n",
    "        (\"user\", template_str),\n",
    "    ])\n",
    "    # 모델 초기화 (버전에 따라 model 혹은 model_name 사용)\n",
    "    try:\n",
    "        llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "    except TypeError:\n",
    "        llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "    return prompt | llm | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [basic] @ gpt-4.1-nano (temp=0.6) ===\n",
      "영어 회화:\n",
      "1. Customer: Hi, I’d like a large latte, please.\n",
      "2. Barista: Sure, would you like any adjustments?\n",
      "3. Customer: Yes, could you make it with less syrup? It’s a bit too sweet for me.\n",
      "4. Barista: Of course. Would you like me to use less sugar or fewer sweeteners?\n",
      "5. Customer: Just less syrup, please. Thank you.\n",
      "6. Barista: No problem. I’ll have that ready for you shortly.\n",
      "\n",
      "한글 해석:\n",
      "1. 고객: 안녕하세요, 라지 라떼 하나 주세요.\n",
      "2. 바리스타: 네, 조절 사항이 있으신가요?\n",
      "3. 고객: 네, 시럽을 적게 넣어 주세요. 너무 달아서요.\n",
      "4. 바리스타: 알겠습니다. 설탕을 적게 넣거나 다른 단맛 재료를 덜 넣어 드릴까요?\n",
      "5. 고객: 그냥 시럽만 적게 넣어 주세요. 감사합니다.\n",
      "6. 바리스타: 문제없어요. 곧 준비해 드리겠습니다.\n",
      "\n",
      "=== [target] @ gpt-4.1-nano (temp=0.6) ===\n",
      "영어 회화:  \n",
      "Barista: Good afternoon! Welcome to Green Leaf Café. May I take your order?  \n",
      "Customer: Hi! Yes, I’d like a caramel latte, please. Could you make it with less syrup? It tends to be too sweet for me.  \n",
      "Barista: Of course! I could come up with a special blend to reduce the sweetness. Would you like any other modifications?  \n",
      "Customer: That sounds great. No, just the less sweetened latte, please. Thank you for your help!  \n",
      "Barista: You're welcome! I’ll prepare that right away. Please wait a moment.  \n",
      "Customer: Thank you very much. I appreciate your polite service.\n",
      "\n",
      "한글 해석:  \n",
      "바리스타: 안녕하세요! 그린 리프 카페에 오신 것을 환영합니다. 주문하시겠어요?  \n",
      "손님: 안녕하세요! 네, 카라멜 라떼 하나 주세요. 설탕은 좀 적게 넣어주실 수 있나요? 너무 달게 느껴지거든요.  \n",
      "바리스타: 물론이죠! 설탕의 단맛을 줄이기 위해 특별 블렌드를 생각해 볼 수 있습니다. 다른 수정 사항이 있으신가요?  \n",
      "손님: 그거면 충분해요. 그냥 덜 달게 만들어 주세요. 도와주셔서 감사합니다!  \n",
      "바리스타: 천만에요! 바로 준비하겠습니다. 잠시만 기다려 주세요.  \n",
      "손님: 매우 감사합니다. 정중한 서비스에 감사드려요.\n",
      "\n",
      "사용된 표현 체크리스트:  \n",
      "- polite requests (e.g., \"Could you make it...\", \"Would you like...?\")  \n",
      "- \"could you ~\" (used to make polite requests)  \n",
      "- present perfect (\"I could come up with...\")  \n",
      "- phrasal verb \"come up with\" (used here as \"come up with a special blend\")\n",
      "\n",
      "=== [basic] @ gpt-4.1-mini (temp=0.6) ===\n",
      "- 영어 회화:  \n",
      "Barista: Hi there! What can I get for you today?  \n",
      "Customer: Hi! I’d like a medium iced latte, please.  \n",
      "Barista: Sure! Would you like it with regular or less sugar?  \n",
      "Customer: Could you make it with less sugar? I prefer it not too sweet, please.  \n",
      "Barista: Absolutely, I’ll make it less sweet for you. Anything else?  \n",
      "Customer: No, that’s all. Thank you very much!\n",
      "\n",
      "- 한글 해석:  \n",
      "바리스타: 안녕하세요! 오늘 무엇을 주문하시겠어요?  \n",
      "손님: 안녕하세요! 미디엄 아이스 라떼 하나 주세요.  \n",
      "바리스타: 알겠습니다! 일반 당도 드릴까요, 아니면 덜 달게 할까요?  \n",
      "손님: 덜 달게 해주실 수 있을까요? 너무 달지 않게 부탁드려요.  \n",
      "바리스타: 물론이죠, 덜 달게 만들어 드릴게요. 다른 것은 없으세요?  \n",
      "손님: 아니요, 그게 전부예요. 감사합니다!\n",
      "\n",
      "=== [target] @ gpt-4.1-mini (temp=0.6) ===\n",
      "- 영어 회화:  \n",
      "A: Good afternoon! Could you come up with a recommendation for a less sweet iced latte?  \n",
      "B: Of course! I could make it with less syrup if you like. Would that be okay?  \n",
      "A: Yes, that sounds perfect. Also, could you make sure it’s not too cold? I prefer it a bit warmer.  \n",
      "B: No problem at all. Could you let me know if the sweetness is just right when you taste it?  \n",
      "A: Certainly. Thank you for being so accommodating.  \n",
      "B: You’re welcome! I’ll prepare your drink now.\n",
      "\n",
      "- 한글 해석:  \n",
      "A: 안녕하세요! 달지 않은 아이스 라떼 추천해 주실 수 있나요?  \n",
      "B: 물론이죠! 시럽을 적게 넣어서 만들어 드릴 수 있어요. 괜찮으신가요?  \n",
      "A: 네, 완벽해요. 그리고 너무 차갑지 않게 해 주실 수 있나요? 저는 조금 따뜻한 게 좋아요.  \n",
      "B: 전혀 문제없어요. 마시고 나서 단맛이 적당한지 알려 주실 수 있나요?  \n",
      "A: 물론이죠. 이렇게 배려해 주셔서 감사합니다.  \n",
      "B: 천만에요! 지금 음료 준비해 드릴게요.\n",
      "\n",
      "- 사용된 표현 체크리스트:  \n",
      "☑ polite requests  \n",
      "☑ could you ~  \n",
      "☑ phrasal verb 'come up with'  \n",
      "☑ present perfect (used in \"Thank you for being so accommodating.\")\n",
      "\n",
      "=== [basic] @ gpt-4o-mini (temp=0.6) ===\n",
      "**영어 회화:**\n",
      "\n",
      "1. Customer: Hi there! I’d like to order a medium latte, please.\n",
      "2. Barista: Sure! Would you like any flavoring with that?\n",
      "3. Customer: No, thank you. Just the regular latte, but could you make it less sweet?\n",
      "4. Barista: Absolutely! I can adjust the sweetness for you.\n",
      "5. Customer: That would be great! I prefer my drinks not too sweet.\n",
      "6. Barista: No problem! I’ll have that ready for you in just a moment.\n",
      "\n",
      "**한글 해석:**\n",
      "\n",
      "1. 고객: 안녕하세요! 미디엄 라떼 하나 주문할게요.\n",
      "2. 바리스타: 네, 어떤 맛을 추가할까요?\n",
      "3. 고객: 아니요, 괜찮아요. 그냥 일반 라떼로 주문할 건데, 덜 달게 해주실 수 있나요?\n",
      "4. 바리스타: 물론이죠! 단맛을 조절해드릴 수 있어요.\n",
      "5. 고객: 그거 좋네요! 저는 음료가 너무 달지 않기를 원해요.\n",
      "6. 바리스타: 문제 없어요! 잠시 후에 준비해드릴게요.\n",
      "\n",
      "=== [target] @ gpt-4o-mini (temp=0.6) ===\n",
      "**영어 회화:**\n",
      "\n",
      "1. A: Hi there! Could I please get a medium latte?\n",
      "2. B: Sure! Would you like it sweetened?\n",
      "3. A: Actually, could you make it less sweet? I prefer it that way.\n",
      "4. B: Of course! I’ll make sure it’s not too sweet. Anything else?\n",
      "5. A: Yes, could you also add a sprinkle of cinnamon on top?\n",
      "6. B: Absolutely! Your order will be ready shortly.\n",
      "\n",
      "---\n",
      "\n",
      "**한글 해석:**\n",
      "\n",
      "1. A: 안녕하세요! 중간 사이즈 라떼 하나 주세요.\n",
      "2. B: 물론이죠! 단맛을 원하십니까?\n",
      "3. A: 사실, 덜 달게 해주실 수 있을까요? 저는 그렇게 하는 게 더 좋아요.\n",
      "4. B: 물론입니다! 너무 달지 않게 만들어 드릴게요. 다른 건 더 필요하신가요?\n",
      "5. A: 네, 위에 시나몬 가루 조금 추가해 주실 수 있나요?\n",
      "6. B: 물론입니다! 주문은 곧 준비될 거예요.\n",
      "\n",
      "---\n",
      "\n",
      "**사용된 표현 체크리스트:**\n",
      "- polite requests: \"Could I please get...\", \"Could you make it less sweet?\", \"Could you also add...\"\n",
      "- 'could you ~': \"Could you make it less sweet?\", \"Could you also add...\"\n"
     ]
    }
   ],
   "source": [
    "TEMPLATES = {\n",
    "    \"basic\": TEMPLATE_BASIC,\n",
    "    \"target\": TEMPLATE_TARGET,\n",
    "    \"rubric\": TEMPLATE_RUBRIC,\n",
    "    \"json\": TEMPLATE_JSON,\n",
    "}\n",
    "\n",
    "MODELS = [\n",
    "    \"gpt-4.1-nano\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-4o-mini\",\n",
    "]\n",
    "\n",
    "def run_experiment(question, level=\"중급\", turns=6, targets=\"polite requests\", cefr=\"B1\",\n",
    "                   template_key=\"basic\", model_name=\"gpt-4.1-nano\", temperature=0.7):\n",
    "    chain = build_chain(TEMPLATES[template_key], model_name, temperature)\n",
    "    inputs = {\n",
    "        \"question\": question,\n",
    "        \"level\": level,\n",
    "        \"turns\": turns,\n",
    "        \"targets\": targets,\n",
    "        \"cefr\": cefr,\n",
    "    }\n",
    "    result = chain.invoke(inputs)\n",
    "    print(f\"\\n=== [{template_key}] @ {model_name} (temp={temperature}) ===\")\n",
    "    print(result[:2000])  # 출력 너무 길면 앞부분만 미리보기\n",
    "    return result\n",
    "\n",
    "# 예시 실행\n",
    "question = \"카페에서 주문하고, 음료가 너무 달지 않게 해달라고 정중히 부탁하는 상황\"\n",
    "for m in MODELS:\n",
    "    for t in [\"basic\", \"target\"]:\n",
    "        run_experiment(question, level=\"중급\", turns=6, targets=\"polite requests, 'could you ~'\", template_key=t, model_name=m, temperature=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인을 구성합니다.\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화:  \n",
      "\"Hello! I’d like to order a table for two, please.\"  \n",
      "\"Could I see the menu, please?\"  \n",
      "\"I’ll have the grilled chicken salad and a glass of water.\"  \n",
      "\"Can I get the dressing on the side?\"  \n",
      "\"Thank you. That will be all for now.\"\n",
      "\n",
      "- 한글 해석:  \n",
      "\"안녕하세요! 2인용 테이블 예약하고 싶어요.\"  \n",
      "\"메뉴 좀 보여주시겠어요?\"  \n",
      "\"그릴드 치킨 샐러드랑 물 한 잔 주세요.\"  \n",
      "\"드레싱은 따로 주시겠어요?\"  \n",
      "\"감사합니다. 지금은 이것으로 주문 끝입니다.\"\n"
     ]
    }
   ],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "print(chain.invoke({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화:  \n",
      "\"Hello, I would like to order a meal, please.\"  \n",
      "\"Hi, could I get the menu, please?\"  \n",
      "\"Can I have the [specific dish] and a glass of water?\"  \n",
      "\n",
      "- 한글 해석:  \n",
      "\"안녕하세요, 음식을 주문하고 싶어요.\"  \n",
      "\"안녕하세요, 메뉴판 좀 주시겠어요?\"  \n",
      "\"[특정 요리] 하나와 물 한 잔 주세요.\""
     ]
    }
   ],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화:  \n",
      "**Customer:** Hi, I’d like to order a large pepperoni pizza, please.  \n",
      "**Employee:** Sure! Would you like curbside pickup or delivery?  \n",
      "**Customer:** Delivery, please. How long will it take?  \n",
      "**Employee:** It will be about 30 minutes. Can I have your address?  \n",
      "**Customer:** Yes, it’s 123 Maple Street.  \n",
      "**Employee:** Great. Your total is $15.99. Would you like to add any drinks or sides?  \n",
      "**Customer:** No, that’s all. Thank you!  \n",
      "**Employee:** Thank you! Your pizza will arrive soon.  \n",
      "\n",
      "- 한글 해석:  \n",
      "**손님:** 안녕하세요, 대형 페퍼로니 피자 하나 주문할게요.  \n",
      "**직원:** 네! 포장하실 건가요 아니면 배달을 원하시나요?  \n",
      "**손님:** 배달로 할게요. 얼마나 걸릴까요?  \n",
      "**직원:** 약 30분 정도 걸립니다. 주소를 알려주시겠어요?  \n",
      "**손님:** 네, 123 메이플 스트리트입니다.  \n",
      "**직원:** 좋아요. 총 금액은 15.99달러입니다. 음료수나 사이드 메뉴도 추가하시겠어요?  \n",
      "**손님:** 아니요, 그게 다예요. 감사합니다!  \n",
      "**직원:** 감사합니다! 곧 피자가 배달될 거예요."
     ]
    }
   ],
   "source": [
    "# 이번에는 question 을 '미국에서 피자 주문'으로 설정하여 실행합니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"미국에서 피자 주문\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-Us6BDj1P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
